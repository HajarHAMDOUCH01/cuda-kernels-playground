{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sh-pdQzFmNX"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRqhp9ipFyuR",
        "outputId": "c07fd8e0-9b1f-4106-fe19-9adb3b375e31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting parallel_reduction_seq_addressing.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile parallel_reduction_seq_addressing.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void initVectors(int *vector, int n){\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < n){ // each thread initializes an element in the vector\n",
        "        vector[idx] = idx;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void reduce_seq_addressing(int *g_in_data, int *g_out_data, int n){\n",
        "    extern __shared__ int sdata[];  // stored in the shared memory\n",
        "\n",
        "    // Each thread loading one element from global onto shared memory\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Load or pad with 0 (handle edge cases where n is not a multiple of blockDim)\n",
        "    sdata[tid] = (i < n) ? g_in_data[i] : 0;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Reduction method -- occurs in shared memory\n",
        "    for(unsigned int s = blockDim.x/2; s > 0; s >>= 1){\n",
        "        // reduce_seq_addressing -- check out the reverse loop above\n",
        "        if (tid < s){   // then, we check threadID to do our computation\n",
        "            sdata[tid] += sdata[tid + s];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "    if (tid == 0){\n",
        "        g_out_data[blockIdx.x] = sdata[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    int N = 50000000;\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    int n = N;\n",
        "\n",
        "    // CUDA events for timing\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    int *d_vector;\n",
        "    cudaMalloc(&d_vector, N * sizeof(int));\n",
        "\n",
        "    // Time initialization\n",
        "    cudaEventRecord(start);\n",
        "    initVectors<<<blocksPerGrid, threadsPerBlock>>>(d_vector, N);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float initTime = 0;\n",
        "    cudaEventElapsedTime(&initTime, start, stop);\n",
        "\n",
        "    int *d_in = d_vector; \n",
        "    int *d_out;\n",
        "\n",
        "    // Time reduction\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    int totalIterations = 0;\n",
        "    while (n > 1) {\n",
        "        int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
        "\n",
        "        cudaMalloc(&d_out, blocksPerGrid * sizeof(int));\n",
        "\n",
        "        reduce_seq_addressing<<<blocksPerGrid, threadsPerBlock,\n",
        "                threadsPerBlock * sizeof(int)>>>(d_in, d_out, n);\n",
        "\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        if (d_in != d_vector)\n",
        "            cudaFree(d_in);\n",
        "\n",
        "        d_in = d_out;\n",
        "        n = blocksPerGrid;\n",
        "        totalIterations++;\n",
        "    }\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float reduce_seq_addressingTime = 0;\n",
        "    cudaEventElapsedTime(&reduce_seq_addressingTime, start, stop);\n",
        "\n",
        "    int result;\n",
        "    cudaMemcpy(&result, d_in, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Calculate bandwidth\n",
        "    int totalDataRead = N * sizeof(int); // Initial read\n",
        "    int totalDataWritten = N * sizeof(int); // Writes during reduction\n",
        "    float totalDataGB = (totalDataRead + totalDataWritten) / (1024.0f * 1024.0f * 1024.0f);\n",
        "    float totalTimeSeconds = (initTime + reduce_seq_addressingTime) / 1000.0f;\n",
        "    float bandwidthGBps = totalDataGB / totalTimeSeconds;\n",
        "\n",
        "    // Results\n",
        "    std::cout << \"========================================\" << std::endl;\n",
        "    std::cout << \"CUDA Parallel Reduction Profiling\" << std::endl;\n",
        "    std::cout << \"========================================\" << std::endl;\n",
        "    std::cout << \"Array Size (N):          \" << N << std::endl;\n",
        "    std::cout << \"Threads Per Block:       \" << threadsPerBlock << std::endl;\n",
        "    std::cout << \"Reduction Iterations:    \" << totalIterations << std::endl;\n",
        "    std::cout << \"----------------------------------------\" << std::endl;\n",
        "    std::cout << \"Initialization Time:     \" << initTime << \" ms\" << std::endl;\n",
        "    std::cout << \"Reduction Time:          \" << reduce_seq_addressingTime << \" ms\" << std::endl;\n",
        "    std::cout << \"Total Time:              \" << (initTime + reduce_seq_addressingTime) << \" ms\" << std::endl;\n",
        "    std::cout << \"----------------------------------------\" << std::endl;\n",
        "    std::cout << \"Effective Bandwidth:     \" << bandwidthGBps << \" GB/s\" << std::endl;\n",
        "    std::cout << \"========================================\" << std::endl;\n",
        "    std::cout << \"Sum Result:              \" << result << std::endl;\n",
        "    std::cout << \"Expected:  \" << (N * (N - 1) / 2) << std::endl;\n",
        "    std::cout << \"========================================\" << std::endl;\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_vector);\n",
        "    cudaFree(d_in);\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgnV1j4nF6GH",
        "outputId": "469f8348-ce1e-4d20-e409-15d738e8bf79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================\n",
            "CUDA Parallel Reduction Profiling\n",
            "========================================\n",
            "Array Size (N):          50000000\n",
            "Threads Per Block:       256\n",
            "Reduction Iterations:    4\n",
            "----------------------------------------\n",
            "Initialization Time:     0.956416 ms\n",
            "Reduction Time:          4.99706 ms\n",
            "Total Time:              5.95347 ms\n",
            "----------------------------------------\n",
            "Effective Bandwidth:     62.5734 GB/s\n",
            "========================================\n",
            "Sum Result:              1283106752\n",
            "Expected:  -864376896\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "!nvcc -arch=sm_75 parallel_reduction_seq_addressing.cu -o parallel_reduction_seq_addressing\n",
        "!./parallel_reduction_seq_addressing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhxtSlnJPqFm"
      },
      "source": [
        "**the improvement of computational time shows with bigger input array sizes** (5.95 ms for 50million input size) and interleaved addressing 8.61491 ms"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
