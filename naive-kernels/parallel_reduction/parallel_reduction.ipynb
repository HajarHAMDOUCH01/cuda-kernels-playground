{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sh-pdQzFmNX"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRqhp9ipFyuR",
        "outputId": "304fe0df-224a-4053-c987-14482a4e9ac4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting parallel_reduction.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile parallel_reduction.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void initVectors(int *vector, int n){\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < n){ // each thread initializes an element in the vector\n",
        "        vector[idx] = idx;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void reduce(int *g_in_data, int *g_out_data, int n) {\n",
        "    extern __shared__ int sdata[];\n",
        "\n",
        "    unsigned int tid = threadIdx.x;// this is the thread's idx in the block it is in\n",
        "    unsigned int i = blockIdx.x * blockDim.x + tid;// this is for tracking elements of the data within the block the thread is in\n",
        "\n",
        "    // Load or pad with 0\n",
        "    sdata[tid] = (i < n) ? g_in_data[i] : 0;// each thread copies elements of its block -IN PARALLEL : in threads and in blocks-\n",
        "    __syncthreads();\n",
        "\n",
        "    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n",
        "        if (tid % (2 * s) == 0) {// threads within the block are multiplies of 2 , 4 , etc\n",
        "            sdata[tid] += sdata[tid + s];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {// the element 0 of each block has the reduction output\n",
        "        g_out_data[blockIdx.x] = sdata[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    int N = 1000;\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    int n = N;\n",
        "\n",
        "    // CUDA events for timing\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    int *d_vector;\n",
        "    cudaMalloc(&d_vector, N * sizeof(int));\n",
        "\n",
        "    // Time initialization\n",
        "    cudaEventRecord(start);\n",
        "    initVectors<<<blocksPerGrid, threadsPerBlock>>>(d_vector, N);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float initTime = 0;\n",
        "    cudaEventElapsedTime(&initTime, start, stop);\n",
        "\n",
        "    int *d_in = d_vector; \n",
        "    int *d_out;\n",
        "\n",
        "    // Time reduction\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    int totalIterations = 0;\n",
        "    while (n > 1) {\n",
        "        int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
        "\n",
        "        cudaMalloc(&d_out, blocksPerGrid * sizeof(int));\n",
        "\n",
        "        reduce<<<blocksPerGrid, threadsPerBlock,\n",
        "                threadsPerBlock * sizeof(int)>>>(d_in, d_out, n);\n",
        "\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        if (d_in != d_vector)\n",
        "            cudaFree(d_in);\n",
        "\n",
        "        d_in = d_out;\n",
        "        n = blocksPerGrid;\n",
        "        totalIterations++;\n",
        "    }\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float reduceTime = 0;\n",
        "    cudaEventElapsedTime(&reduceTime, start, stop);\n",
        "\n",
        "    int result;\n",
        "    cudaMemcpy(&result, d_in, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    int totalDataRead = N * sizeof(int); // Initial read\n",
        "    int totalDataWritten = N * sizeof(int); // Writes during reduction\n",
        "    float totalDataGB = (totalDataRead + totalDataWritten) / (1024.0f * 1024.0f * 1024.0f);\n",
        "    float totalTimeSeconds = (initTime + reduceTime) / 1000.0f;\n",
        "    float bandwidthGBps = totalDataGB / totalTimeSeconds;\n",
        "\n",
        "    // Results\n",
        "    std::cout << \"========================================\" << std::endl;\n",
        "    std::cout << \"CUDA Parallel Reduction Profiling\" << std::endl;\n",
        "    std::cout << \"========================================\" << std::endl;\n",
        "    std::cout << \"Array Size (N):          \" << N << std::endl;\n",
        "    std::cout << \"Threads Per Block:       \" << threadsPerBlock << std::endl;\n",
        "    std::cout << \"Reduction Iterations:    \" << totalIterations << std::endl;\n",
        "    std::cout << \"----------------------------------------\" << std::endl;\n",
        "    std::cout << \"Initialization Time:     \" << initTime << \" ms\" << std::endl;\n",
        "    std::cout << \"Reduction Time:          \" << reduceTime << \" ms\" << std::endl;\n",
        "    std::cout << \"Total Time:              \" << (initTime + reduceTime) << \" ms\" << std::endl;\n",
        "    std::cout << \"----------------------------------------\" << std::endl;\n",
        "    std::cout << \"Effective Bandwidth:     \" << bandwidthGBps << \" GB/s\" << std::endl;\n",
        "    std::cout << \"========================================\" << std::endl;\n",
        "    std::cout << \"Sum Result:              \" << result << std::endl;\n",
        "    std::cout << \"Expected:  \" << (N * (N - 1) / 2) << std::endl;\n",
        "    std::cout << \"========================================\" << std::endl;\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_vector);\n",
        "    cudaFree(d_in);\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgnV1j4nF6GH",
        "outputId": "61405ee9-2d03-4d4b-f765-b72c4f1ee5cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================\n",
            "CUDA Parallel Reduction Profiling\n",
            "========================================\n",
            "Array Size (N):          1000\n",
            "Threads Per Block:       256\n",
            "Reduction Iterations:    2\n",
            "----------------------------------------\n",
            "Initialization Time:     0.13552 ms\n",
            "Reduction Time:          0.05408 ms\n",
            "Total Time:              0.1896 ms\n",
            "----------------------------------------\n",
            "Effective Bandwidth:     0.0392963 GB/s\n",
            "========================================\n",
            "Sum Result:              499500\n",
            "Expected:  499500\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "!nvcc -arch=sm_75 parallel_reduction.cu -o parallel_reduction\n",
        "!./parallel_reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecAQGZ7uGA_Y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
